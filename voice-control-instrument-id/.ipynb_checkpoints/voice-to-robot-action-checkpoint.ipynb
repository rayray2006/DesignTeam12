{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ac83fc-fad7-4ba8-bd93-82cc40f2942b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loading model finished\n",
      "Using prompts: Scalpel Forceps Scissors Tweezers\n",
      "Detected wakeword\n",
      "is_recording: False\n",
      " Give me scissors\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pvporcupine\n",
    "import pvcobra\n",
    "import whisper\n",
    "from pvrecorder import PvRecorder\n",
    "import torch\n",
    "import string\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# frame length = 512\n",
    "# samples per frame = 16,000\n",
    "# 1 sec = 16,000 / 512\n",
    "\n",
    "\n",
    "class Transcriber:\n",
    "    def __init__(self, model) -> None:\n",
    "        print(\"loading model\")\n",
    "        # TODO: put model on GPU\n",
    "        self.model = whisper.load_model(model)\n",
    "        print(\"loading model finished\")\n",
    "        self.prompts = os.environ.get(\"WHISPER_INITIAL_PROMPT\", \"\")\n",
    "        print(f\"Using prompts: {self.prompts}\")\n",
    "\n",
    "    def transcribe(self, frames):\n",
    "        transcribe_start = time.time()\n",
    "        samples = np.array(frames, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "        # audio = whisper.pad_or_trim(samples)\n",
    "        # print(f\"{transcribe_start} transcribing {len(frames)} frames.\")\n",
    "        # # audio = whisper.pad_or_trim(frames)\n",
    "\n",
    "        # # make log-Mel spectrogram and move to the same device as the model\n",
    "        # mel = whisper.log_mel_spectrogram(audio).to(self.model.device)\n",
    "\n",
    "        # # decode the audio\n",
    "        # options = whisper.DecodingOptions(fp16=False, language=\"english\")\n",
    "        # result = whisper.decode(self.model, mel, options)\n",
    "\n",
    "        result = self.model.transcribe(\n",
    "            audio=samples,\n",
    "            language=\"en\",\n",
    "            fp16=False,\n",
    "            initial_prompt=self.prompts,\n",
    "        )\n",
    "\n",
    "        # print the recognized text\n",
    "        transcribe_end = time.time()\n",
    "        # print(\n",
    "        #     f\"{transcribe_end} - {transcribe_end - transcribe_start}sec: {result.get('text')}\",\n",
    "        #     flush=True,\n",
    "        # )\n",
    "        return result.get(\"text\", \"speech not detected\")\n",
    "\n",
    "def initialize_voice():\n",
    "    load_dotenv()\n",
    "\n",
    "    porcupine = pvporcupine.create(\n",
    "        access_key=os.environ.get(\"ACCESS_KEY\"),\n",
    "        keyword_paths=[os.environ.get(\"WAKE_WORD_MODEL_PATH\")],\n",
    "    )\n",
    "    \n",
    "    cobra = pvcobra.create(\n",
    "        access_key=os.environ.get(\"ACCESS_KEY\"),\n",
    "    )\n",
    "\n",
    "    recoder = PvRecorder(device_index=-1, frame_length=512)\n",
    "\n",
    "def get_voice_command():\n",
    "    \n",
    "    transcriber = Transcriber(os.environ.get(\"WHISPER_MODEL\"))\n",
    "    \n",
    "    sample_rate = 16000\n",
    "    frame_size = 512\n",
    "    vad_mean_probability_sensitivity = float(os.environ.get(\"VAD_SENSITIVITY\"))\n",
    "    \n",
    "    recoder.start()\n",
    "    \n",
    "    max_window_in_secs = 3\n",
    "    window_size = sample_rate * max_window_in_secs\n",
    "    samples = deque(maxlen=(window_size * 6))\n",
    "    vad_samples = deque(maxlen=25)\n",
    "    is_recording = False\n",
    "    \n",
    "    while True:\n",
    "        data = recoder.read()\n",
    "        vad_prob = cobra.process(data)\n",
    "        vad_samples.append(vad_prob)\n",
    "        # print(f\"{vad_prob} - {np.mean(vad_samples)} - {len(vad_samples)}\")\n",
    "        if porcupine.process(data) >= 0:\n",
    "            print(f\"Detected wakeword\")\n",
    "            is_recording = True\n",
    "            samples.clear()\n",
    "    \n",
    "        if is_recording:\n",
    "            if (\n",
    "                len(samples) < window_size\n",
    "                or np.mean(vad_samples) >= vad_mean_probability_sensitivity\n",
    "            ):\n",
    "                samples.extend(data)\n",
    "                # print(f\"listening - samples: {len(samples)}\")\n",
    "            else:\n",
    "                print(\"is_recording: False\")\n",
    "                print(transcriber.transcribe(samples))\n",
    "                is_recording = False\n",
    "                recoder.stop()\n",
    "                porcupine.delete()\n",
    "                recoder.delete()\n",
    "                cobra.delete()\n",
    "                break\n",
    "    \n",
    "    command = transcriber.transcribe(samples)\n",
    "\n",
    "    return command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43138053-d5d3-4cae-8814-d0cd345bce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument: scissors\n"
     ]
    }
   ],
   "source": [
    "# Identify instrument from transcription\n",
    "\n",
    "def transcribe_instrument(command):\n",
    "    # command = transcriber.transcribe(samples)\n",
    "    # command = \"Give me forceps\"\n",
    "    instruments = ['forceps', 'scalpel', 'scissors', 'tweezers']\n",
    "    \n",
    "    instrument = ''\n",
    "    for word in command.split():\n",
    "        word = (word.translate(str.maketrans('', '', string.punctuation))).lower()\n",
    "        if word in instruments:\n",
    "            instrument = word\n",
    "    \n",
    "    if instrument == '':\n",
    "        print(\"No instrument found\")\n",
    "    else:\n",
    "        print(\"Instrument: \" + instrument)\n",
    "\n",
    "    return instrument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4debb061-0a4f-4e5c-b169-7fad517d3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\maxis/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-6 Python-3.9.21 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Yolo trained on DocCheck (Rona) dataset\n",
    "\n",
    "def load_model():\n",
    "    # load yolov5 from online\n",
    "    # TODO: change to local\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/instrument_detector_model.pt', force_reload=True)  # load a custom model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd98588-05c4-485d-a50d-bc110aef6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play depth camera, save image as png\n",
    "\n",
    "def get_camera_img:\n",
    "    # Setup pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    # Enable both depth and color streams\n",
    "    # config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    # Start streaming\n",
    "    pipeline.start(config)\n",
    "    try:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        # depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            print(\"Failed to get frames\")\n",
    "        else:\n",
    "            # Convert images to numpy arrays\n",
    "            # depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            # Apply color map to depth image for visualization\n",
    "            # depth_colormap = cv2.applyColorMap(\n",
    "            #     cv2.convertScaleAbs(depth_image, alpha=0.03),\n",
    "            #     cv2.COLORMAP_JET\n",
    "            # )\n",
    "            # Stack both images side-by-side\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "            # Display the images\n",
    "            cv2.imshow('Color', images)\n",
    "            cv2.waitKey(0)\n",
    "            # Optional: Save to file\n",
    "            cv2.imwrite(\"inst-camera.png\", color_image)\n",
    "            # cv2.imwrite(\"depth_colormap.png\", depth_colormap)\n",
    "    finally:\n",
    "        # Stop streaming\n",
    "        pipeline.stop()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2e429-2881-48e9-bf0a-81f911352520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and translate results\n",
    "\n",
    "def identify_instrument(img_path):\n",
    "\n",
    "    # inference\n",
    "    results = model(img_path)\n",
    "    \n",
    "    # Translate to English\n",
    "    results.names = {0: 'Standard Anatomical Tweezers',\n",
    "     1: 'Slim Anatomical Tweezers',\n",
    "     2: 'Surgical Tweezers',\n",
    "     3: 'Splinter Tweezers',\n",
    "     4: 'Scalpel Handle No. 3',\n",
    "     5: 'Scalpel Handle No. 4',\n",
    "     6: 'Clenched Scalpel',\n",
    "     7: 'Narrow Scalpel',\n",
    "     8: 'Surgical Scissors Sharp/Sharp',\n",
    "     9: 'Surgical Scissors Sharp/Narrow',\n",
    "     10: 'Standard Dissecting Scissors',\n",
    "     11: 'Dissecting Needle'}\n",
    "    \n",
    "    # map labels to basic voice commands \"forceps\", \"scalpel\", \"scissors\", \"needle\"\n",
    "    map_instruments = {results.names[0]: 'forceps',\n",
    "                       results.names[1]: 'forceps',\n",
    "                       results.names[2]: 'forceps',\n",
    "                       results.names[3]: 'forceps',\n",
    "                       results.names[4]: 'scalpel',\n",
    "                       results.names[5]: 'scalpel',\n",
    "                       results.names[6]: 'scalpel',\n",
    "                       results.names[7]: 'scalpel',\n",
    "                       results.names[8]: 'scissors',\n",
    "                       results.names[9]: 'scissors',\n",
    "                       results.names[10]: 'scissors',\n",
    "                       results.names[11]: 'needle'}\n",
    "    # get midpoint\n",
    "\n",
    "    detections = results.xyxy[0]\n",
    "    \n",
    "    conf_threshold = 0.65\n",
    "    # instruments = 'forceps'\n",
    "    \n",
    "    # TODO: account for when there's multiple types of the same instrument\n",
    "    \n",
    "    x_midpoint = 0\n",
    "    y_midpoint = 0\n",
    "    \n",
    "    for *box, conf, cls in detections:\n",
    "        cls = int(cls)\n",
    "        x1, y1, x2, y2 = box\n",
    "        # if detection matches the instrument from voice command\n",
    "        if map_instruments[results.names[cls]] == instrument and conf >= conf_threshold :\n",
    "            x_midpoint = (x1 + x2) / 2\n",
    "            y_midpoint = (y1 + y2) / 2\n",
    "            print(f\"Box center: ({x_midpoint:.0f}, {y_midpoint:.0f}) | Confidence: {conf:.2f} | Class: {results.names[cls]}\")\n",
    "            break\n",
    "    \n",
    "    if x_midpoint == 0:\n",
    "        print(\"No instrument found\")\n",
    "\n",
    "    return x_midpoint, y_midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acdc1e78-dfd2-4d15-ba27-f6233739e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (1946, 1560) | Confidence: 0.79 | Class: Surgical Tweezers\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dt12)",
   "language": "python",
   "name": "dt12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
