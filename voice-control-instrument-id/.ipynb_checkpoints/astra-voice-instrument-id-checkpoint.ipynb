{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ac83fc-fad7-4ba8-bd93-82cc40f2942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loading model finished\n",
      "Using prompts: Scalpel Forceps Scissors\n",
      "Detected wakeword\n",
      "is_recording: False\n",
      " Give me a scalpel.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pvporcupine\n",
    "import pvcobra\n",
    "import whisper\n",
    "from pvrecorder import PvRecorder\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "porcupine = pvporcupine.create(\n",
    "    access_key=os.environ.get(\"ACCESS_KEY\"),\n",
    "    keyword_paths=[os.environ.get(\"WAKE_WORD_MODEL_PATH\")],\n",
    ")\n",
    "\n",
    "cobra = pvcobra.create(\n",
    "    access_key=os.environ.get(\"ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "recoder = PvRecorder(device_index=-1, frame_length=512)\n",
    "\n",
    "# frame length = 512\n",
    "# samples per frame = 16,000\n",
    "# 1 sec = 16,000 / 512\n",
    "\n",
    "\n",
    "class Transcriber:\n",
    "    def __init__(self, model) -> None:\n",
    "        print(\"loading model\")\n",
    "        # TODO: put model on GPU\n",
    "        self.model = whisper.load_model(model)\n",
    "        print(\"loading model finished\")\n",
    "        self.prompts = os.environ.get(\"WHISPER_INITIAL_PROMPT\", \"\")\n",
    "        print(f\"Using prompts: {self.prompts}\")\n",
    "\n",
    "    def transcribe(self, frames):\n",
    "        transcribe_start = time.time()\n",
    "        samples = np.array(frames, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "\n",
    "        # audio = whisper.pad_or_trim(samples)\n",
    "        # print(f\"{transcribe_start} transcribing {len(frames)} frames.\")\n",
    "        # # audio = whisper.pad_or_trim(frames)\n",
    "\n",
    "        # # make log-Mel spectrogram and move to the same device as the model\n",
    "        # mel = whisper.log_mel_spectrogram(audio).to(self.model.device)\n",
    "\n",
    "        # # decode the audio\n",
    "        # options = whisper.DecodingOptions(fp16=False, language=\"english\")\n",
    "        # result = whisper.decode(self.model, mel, options)\n",
    "\n",
    "        result = self.model.transcribe(\n",
    "            audio=samples,\n",
    "            language=\"en\",\n",
    "            fp16=False,\n",
    "            initial_prompt=self.prompts,\n",
    "        )\n",
    "\n",
    "        # print the recognized text\n",
    "        transcribe_end = time.time()\n",
    "        # print(\n",
    "        #     f\"{transcribe_end} - {transcribe_end - transcribe_start}sec: {result.get('text')}\",\n",
    "        #     flush=True,\n",
    "        # )\n",
    "        return result.get(\"text\", \"speech not detected\")\n",
    "\n",
    "\n",
    "transcriber = Transcriber(os.environ.get(\"WHISPER_MODEL\"))\n",
    "\n",
    "sample_rate = 16000\n",
    "frame_size = 512\n",
    "vad_mean_probability_sensitivity = float(os.environ.get(\"VAD_SENSITIVITY\"))\n",
    "\n",
    "try:\n",
    "    recoder.start()\n",
    "\n",
    "    max_window_in_secs = 3\n",
    "    window_size = sample_rate * max_window_in_secs\n",
    "    samples = deque(maxlen=(window_size * 6))\n",
    "    vad_samples = deque(maxlen=25)\n",
    "    is_recording = False\n",
    "\n",
    "    while True:\n",
    "        data = recoder.read()\n",
    "        vad_prob = cobra.process(data)\n",
    "        vad_samples.append(vad_prob)\n",
    "        # print(f\"{vad_prob} - {np.mean(vad_samples)} - {len(vad_samples)}\")\n",
    "        if porcupine.process(data) >= 0:\n",
    "            print(f\"Detected wakeword\")\n",
    "            is_recording = True\n",
    "            samples.clear()\n",
    "\n",
    "        if is_recording:\n",
    "            if (\n",
    "                len(samples) < window_size\n",
    "                or np.mean(vad_samples) >= vad_mean_probability_sensitivity\n",
    "            ):\n",
    "                samples.extend(data)\n",
    "                # print(f\"listening - samples: {len(samples)}\")\n",
    "            else:\n",
    "                print(\"is_recording: False\")\n",
    "                print(transcriber.transcribe(samples))\n",
    "                is_recording = False\n",
    "except KeyboardInterrupt:\n",
    "    recoder.stop()\n",
    "finally:\n",
    "    porcupine.delete()\n",
    "    recoder.delete()\n",
    "    cobra.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4debb061-0a4f-4e5c-b169-7fad517d3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\PIL\\Image.py:119: RuntimeWarning: The _imaging extension was built for another version of Pillow or PIL:\n",
      "Core version: 11.1.0\n",
      "Pillow version: 10.4.0\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The _imaging extension was built for another version of Pillow or PIL:\nCore version: 11.1.0\nPillow version: 10.4.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./last.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the image and preprocess it\u001b[39;00m\n\u001b[0;32m     26\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test_instruments.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path):\n\u001b[1;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\torch\\serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\torch\\serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\torch\\serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\ultralytics\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# default for reduced CPU utilization during training\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NAS, RTDETR, SAM, YOLO, YOLOE, FastSAM, YOLOWorld\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASSETS, SETTINGS\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_yolo \u001b[38;5;28;01mas\u001b[39;00m checks\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\ultralytics\\models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastsam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NAS\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrtdetr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\ultralytics\\models\\fastsam\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMPredictor\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMValidator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\ultralytics\\models\\fastsam\\model.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMPredictor\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAMValidator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\ultralytics\\engine\\model.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcfg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TASK2DATA, get_cfg, get_save_dir\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresults\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Results\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dt12\\lib\\site-packages\\PIL\\Image.py:105\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m v:\n",
      "\u001b[1;31mImportError\u001b[0m: The _imaging extension was built for another version of Pillow or PIL:\nCore version: 11.1.0\nPillow version: 10.4.0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the image before feeding it to the model\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    # img = img.resize((224, 224))  # Resize to the model's expected input size\n",
    "    img = np.array(img).astype(np.float32) / 255.0  # Convert to a NumPy array and normalize\n",
    "    img = np.transpose(img, (2, 0, 1))  # Transpose the image to (channels, height, width)\n",
    "    img = np.expand_dims(img, axis=0)  # Add a batch dimension\n",
    "    return torch.tensor(img)\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./last.pt\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Load the image and preprocess it\n",
    "image_path = \"./test_instruments.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Run the model\n",
    "output = model(input_image)\n",
    "\n",
    "#command = transcriber.transcribe(samples)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8246ac-9916-40d3-a4d3-e35c149eeb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dt12)",
   "language": "python",
   "name": "dt12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
