{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74587a89-fa9e-4b64-9681-2e85d8386812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded.\n",
      "Using prompts: \n",
      "Audio loaded with sample rate: 16000\n",
      "Transcribing audio...\n",
      "Transcription result:  Astra, give me scissors.\n",
      "Wake word detected: astra\n",
      "Instrument identified: scissors\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import whisper\n",
    "import string\n",
    "import librosa\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize Whisper Model\n",
    "whisper_model = whisper.load_model(\"base\")  # You can use \"small\", \"medium\", etc.\n",
    "\n",
    "# Initialize Speech Recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Initialize Microphone\n",
    "microphone = sr.Microphone()\n",
    "\n",
    "# Transcription Class using Whisper\n",
    "class Transcriber:\n",
    "    def __init__(self, model) -> None:\n",
    "        print(\"Loading Whisper model...\")\n",
    "        self.model = whisper.load_model(model)\n",
    "        print(\"Model loaded.\")\n",
    "        self.prompts = os.environ.get(\"WHISPER_INITIAL_PROMPT\", \"\")\n",
    "        print(f\"Using prompts: {self.prompts}\")\n",
    "\n",
    "    def transcribe(self, frames):\n",
    "        print(\"Transcribing audio...\")\n",
    "        try:\n",
    "            # Ensure audio is in the right format for Whisper\n",
    "            audio_data = np.array(frames, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            result = self.model.transcribe(audio=audio_data, language=\"en\", fp16=False, initial_prompt=self.prompts)\n",
    "            return result.get(\"text\", \"No speech detected\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {e}\")\n",
    "            return None\n",
    "\n",
    "transcriber = Transcriber(\"base\")  # Use any Whisper model variant\n",
    "\n",
    "# List of potential wake words (or phrases)\n",
    "wake_words = [\"astra\"]\n",
    "\n",
    "# Identify instrument from transcription\n",
    "def identify_instrument(command):\n",
    "    instruments = ['forceps', 'scalpel', 'scissors', 'tweezers']\n",
    "    \n",
    "    instrument = ''\n",
    "    for word in command.split():\n",
    "        word = (word.translate(str.maketrans('', '', string.punctuation))).lower()  # Remove punctuation\n",
    "        if word in instruments:\n",
    "            instrument = word\n",
    "            break  # If one instrument is found, break the loop\n",
    "    \n",
    "    if instrument == '':\n",
    "        print(\"No instrument found\")\n",
    "    else:\n",
    "        print(f\"Instrument identified: {instrument}\")\n",
    "\n",
    "# Process audio file for wake word detection\n",
    "def process_audio_file(file_path):\n",
    "    try:\n",
    "        # Load audio using librosa with resampling to 16kHz\n",
    "        audio_data, sr = librosa.load(file_path, sr=16000)  # Resample to 16kHz if needed\n",
    "        print(f\"Audio loaded with sample rate: {sr}\")\n",
    "\n",
    "        # Ensure audio data is in the correct format for Whisper\n",
    "        audio_data = (audio_data * 32768).astype(np.int16)\n",
    "        \n",
    "        # Transcribe the audio\n",
    "        transcription = transcriber.transcribe(audio_data)\n",
    "        \n",
    "        if transcription is None:\n",
    "            print(\"Transcription failed.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Transcription result: {transcription}\")\n",
    "        \n",
    "        # Check for wake word in the transcription\n",
    "        wake_word_detected = False\n",
    "        for wake_word in wake_words:\n",
    "            if wake_word in transcription.lower():\n",
    "                print(f\"Wake word detected: {wake_word}\")\n",
    "                wake_word_detected = True\n",
    "                break\n",
    "        \n",
    "        # If a wake word is detected, then proceed with identifying instrument\n",
    "        if wake_word_detected:\n",
    "            identify_instrument(transcription)  # Identify instrument from transcription\n",
    "        else:\n",
    "            print(\"No wake word detected. Skipping instrument identification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file: {e}\")\n",
    "\n",
    "# Function to listen for wake word\n",
    "def listen_for_wake_word():\n",
    "    print(\"Listening for wake word...\")\n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        while True:\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                query = recognizer.recognize_google(audio).lower()\n",
    "                print(f\"Recognized speech: {query}\")\n",
    "\n",
    "                if any(wake_word in query for wake_word in wake_words):\n",
    "                    print(f\"Wake word detected: {query}\")\n",
    "                    return True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error recognizing speech: {e}\")\n",
    "                continue\n",
    "\n",
    "# Function to listen for speech after wake word is detected\n",
    "def listen_for_speech():\n",
    "    print(\"Listening for command after wake word...\")\n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        print(\"Recognizing speech...\")\n",
    "        query = recognizer.recognize_google(audio)\n",
    "        print(f\"Recognized text: {query}\")\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        print(f\"Error recognizing speech: {e}\")\n",
    "        return None\n",
    "\n",
    "# Combined loop that listens for wake word and then for instrument command\n",
    "def listen_and_transcribe():\n",
    "    while True:\n",
    "        if listen_for_wake_word():\n",
    "            command = listen_for_speech()\n",
    "            if command:\n",
    "                identify_instrument(command)\n",
    "\n",
    "\n",
    "\n",
    "# Process an audio file (already recorded audio) or continuously listen for new speech\n",
    "# process_audio_file(\"/Users/sreyaskanaparti/Downloads/Levering Hall.wav\")\n",
    "# process_audio_file(\"/Users/sreyaskanaparti/Downloads/Levering Hall 2.wav\")\n",
    "# process_audio_file(\"/Users/sreyaskanaparti/Downloads/Levering Hall 3.wav\")\n",
    "# process_audio_file('/Users/sreyaskanaparti/Downloads/Mason Hall.wav')\n",
    "# process_audio_file('/Users/sreyaskanaparti/Downloads/Mason Hall 3.wav')\n",
    "process_audio_file('/Users/sreyaskanaparti/Downloads/output_with_synthetic_noise_3.wav')\n",
    "\n",
    "# listen_and_transcribe()  # Uncomment this line to continuously listen for wake word and then transcribe speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c00f489c-6451-4b66-a195-33adf02229fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with synthetic background noise saved to output_with_synthetic_noise_3.wav\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def generate_white_noise(duration_ms, sample_rate=16000):\n",
    "    \"\"\"Generate white noise with a given duration in milliseconds.\"\"\"\n",
    "    num_samples = int(duration_ms * sample_rate / 1000)\n",
    "    noise = np.random.normal(0, 1, num_samples)  # White noise\n",
    "    return noise\n",
    "\n",
    "def add_synthetic_noise(input_audio_path, output_audio_path, noise_level=0.1):\n",
    "    # Load the original audio file\n",
    "    original_audio = AudioSegment.from_wav(input_audio_path)\n",
    "    \n",
    "    # Get the original audio data as numpy array\n",
    "    samples = np.array(original_audio.get_array_of_samples(), dtype=np.float32)\n",
    "    \n",
    "    # Generate white noise to match the length of the original audio\n",
    "    noise = generate_white_noise(len(samples) / original_audio.frame_rate * 1000, sample_rate=original_audio.frame_rate)\n",
    "    \n",
    "    # Scale the noise to match the noise level (0.1 for low noise, 1.0 for high noise)\n",
    "    noise *= noise_level * np.max(np.abs(samples))  # Adjust the amplitude of the noise\n",
    "    \n",
    "    # Add the noise to the original audio\n",
    "    noisy_samples = samples + noise\n",
    "    \n",
    "    # Clip values to avoid distortion (ensure the values are within the allowed range for audio)\n",
    "    noisy_samples = np.clip(noisy_samples, -32768, 32767)\n",
    "    \n",
    "    # Convert noisy samples back to audio\n",
    "    noisy_audio = AudioSegment(\n",
    "        noisy_samples.astype(np.int16).tobytes(),\n",
    "        frame_rate=original_audio.frame_rate,\n",
    "        sample_width=original_audio.sample_width,\n",
    "        channels=original_audio.channels\n",
    "    )\n",
    "    \n",
    "    # Export the noisy audio to a new file\n",
    "    noisy_audio.export(output_audio_path, format=\"wav\")\n",
    "    print(f\"Audio with synthetic background noise saved to {output_audio_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_audio = \"/Users/sreyaskanaparti/Downloads/Levering Hall 2.wav\"\n",
    "output_audio = \"output_with_synthetic_noise_3.wav\"\n",
    "\n",
    "add_synthetic_noise(input_audio, output_audio, noise_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4debb061-0a4f-4e5c-b169-7fad517d3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\maxis/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-4 Python-3.9.21 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\maxis/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "# Yolo trained on roboflow dataset\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# load yolov5 from online\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/instrument_detector_model.pt', force_reload=True)  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(\"./eval-images/test_instruments.jpg\")  # predict on an image\n",
    "\n",
    "# # Access the results\n",
    "# for result in results:\n",
    "#     xywh = result.boxes.xywh  # center-x, center-y, width, height\n",
    "#     xywhn = result.boxes.xywhn  # normalized\n",
    "#     xyxy = result.boxes.xyxy  # top-left-x, top-left-y, bottom-right-x, bottom-right-y\n",
    "#     xyxyn = result.boxes.xyxyn  # normalized\n",
    "#     names = [result.names[cls.item()] for cls in result.boxes.cls.int()]  # class name of each box\n",
    "#     confs = result.boxes.conf  # confidence score of each box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e2a1344-4f7b-4193-bcbd-89272d9da10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 1 image to \u001b[1mruns\\detect\\exp3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Translate results\n",
    "results.names = {0: 'Standard Anatomical Tweezers',\n",
    " 1: 'Slim Anatomical Tweezers',\n",
    " 2: 'Surgical Tweezers',\n",
    " 3: 'Splinter Tweezers',\n",
    " 4: 'Scalpel Handle No. 3',\n",
    " 5: 'Scalpel Handle No. 4',\n",
    " 6: 'Clenched Scalpel',\n",
    " 7: 'Narrow Scalpel',\n",
    " 8: 'Surgical Scissors Sharp/Sharp',\n",
    " 9: 'Surgical Scissors Sharp/Narrow',\n",
    " 10: 'Standard Dissecting Scissors',\n",
    " 11: 'Dissecting Needle'}\n",
    "\n",
    "# save to runs/detect/expY\n",
    "results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94a4c32-0ef6-4df3-8d83-501574e8397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels saved to C:/Users/maxis/Desktop/JHU/Term 2/Design Team/Data/HOSPI_Tools_small_yolov5/labels/train\\unique_labels.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the directory containing the txt files.\n",
    "directory = 'C:/Users/maxis/Desktop/JHU/Term 2/Design Team/Data/HOSPI_Tools_small_yolov5/labels/train'  # Change this to your directory path\n",
    "\n",
    "# Use a set to store unique labels.\n",
    "unique_labels = set()\n",
    "\n",
    "# Compile a regex pattern:\n",
    "# This pattern looks for a label (anything ending with an underscore)\n",
    "# immediately followed by one or more digits and then the \".txt\" extension.\n",
    "pattern = re.compile(r'^(.*_)(\\d+)\\.txt$', re.IGNORECASE)\n",
    "\n",
    "# Iterate over the files in the directory.\n",
    "with os.scandir(directory) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file() and entry.name.lower().endswith('.txt'):\n",
    "            match = pattern.match(entry.name)\n",
    "            if match:\n",
    "                label = match.group(1)  # Extract the label part (e.g., \"No4BP_Handle_\")\n",
    "                unique_labels.add(label)\n",
    "\n",
    "# Optionally, sort the labels before writing.\n",
    "sorted_labels = sorted(unique_labels)\n",
    "\n",
    "# Write the unique labels to an output file.\n",
    "output_file = os.path.join(directory, 'unique_labels.txt')\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in sorted_labels:\n",
    "        f.write(label + '\\n')\n",
    "\n",
    "print(f\"Unique labels saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07c4de0-8d6d-43d3-97f9-64a642bec0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference_id': '223ecda7-9992-4069-b2fc-20d634142d5f',\n",
       " 'time': 0.03522089600119216,\n",
       " 'image': {'width': 612, 'height': 408},\n",
       " 'predictions': [{'x': 197.5,\n",
       "   'y': 245.0,\n",
       "   'width': 117.0,\n",
       "   'height': 278.0,\n",
       "   'confidence': 0.8790383338928223,\n",
       "   'class': '6_Babcock_Tissue_Forceps',\n",
       "   'class_id': 0,\n",
       "   'detection_id': 'b6db08a7-032c-46d5-9841-e538ad6e00fa'},\n",
       "  {'x': 69.5,\n",
       "   'y': 221.5,\n",
       "   'width': 131.0,\n",
       "   'height': 333.0,\n",
       "   'confidence': 0.8765726089477539,\n",
       "   'class': '6_Babcock_Tissue_Forceps',\n",
       "   'class_id': 0,\n",
       "   'detection_id': 'aa40e352-8f26-4dce-a9df-058d59b341d5'},\n",
       "  {'x': 317.0,\n",
       "   'y': 273.5,\n",
       "   'width': 94.0,\n",
       "   'height': 221.0,\n",
       "   'confidence': 0.7567417025566101,\n",
       "   'class': '6_Babcock_Tissue_Forceps',\n",
       "   'class_id': 0,\n",
       "   'detection_id': '7ca93634-8cd0-4bb9-9087-9dac9d86b134'},\n",
       "  {'x': 506.5,\n",
       "   'y': 265.5,\n",
       "   'width': 211.0,\n",
       "   'height': 243.0,\n",
       "   'confidence': 0.675891637802124,\n",
       "   'class': '6_Babcock_Tissue_Forceps',\n",
       "   'class_id': 0,\n",
       "   'detection_id': 'dd7c24cf-7442-4593-ac9a-6c219f6b6235'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Online hospitools trained model\n",
    "\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url=\"https://detect.roboflow.com\",\n",
    "    api_key=\"2bIaNvju3XOt5sZdFhyl\"\n",
    ")\n",
    "\n",
    "result = CLIENT.infer(\"./eval-images/inst-7.jpg\", model_id=\"hospitools-data/1\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
