{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument ID, arm, camera, mediapipe set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-21 Python-3.9.21 torch-2.6.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import struct\n",
    "import socket\n",
    "import torch\n",
    "import random\n",
    "# Import modules from voice-control-instrument-id/voice_instrument_functions.py\n",
    "#instrument_module_path = os.path.abspath(os.path.join(__file__, \"..\", \"..\", \"voice-control-instrument-id\"))\n",
    "notebook_path = os.getcwd()  # Current working directory\n",
    "instrument_module_path = os.path.abspath(os.path.join(notebook_path, \"..\", \"voice-control-instrument-id\"))\n",
    "sys.path.append(instrument_module_path) \n",
    "from voice_instrument_functions import *\n",
    "from audio_utils import *\n",
    "\n",
    "# Load instrument identification model\n",
    "inst_model = load_model('../voice-control-instrument-id/models/instrument_detector_model.pt', False)\n",
    "\n",
    "from robot_control_functions import * # Because of mediapipe, need to import AFTER loading instrument id model\n",
    "\n",
    "# Set up MyCoBot 280\n",
    "HOST = \"10.42.0.1\"\n",
    "GET_COORDS_PORT = 5006\n",
    "MOVE_COORDS_PORT = 5005\n",
    "MOVE_GRIPPER_PORT = 5007\n",
    "home = [90, 0, 0, -90, 0, -45]\n",
    "handHome = [[90, 0, 0, 90, 0, -45]]\n",
    "\n",
    "# Set up Mediapipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Set up depth camera\n",
    "# TODO: open as a window for Design Day\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "#inst_img = get_camera_img(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activation set up - ONLY RUN THIS IF YOU WANT TO SET A CUSTOM VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_tts_voice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (403, 182) | Confidence: 0.74 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "Gripper command sent: 0 50\n",
      "Response from robot: \n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "[     36.451     -116.27       290.1]\n",
      "-52.493472186602155\n",
      "Response from robot: \n",
      "[     1.1437     -258.53      98.674      179.43       -0.18     -81.707]\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "[     1.1437     -258.53      98.674      179.43       -0.18     -81.707]\n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n",
      "No hand detected for 10 frames. Sending robot home.\n",
      "Response from robot: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m send_coords(handHome, HOST, MOVE_COORDS_PORT, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mmove_to_hand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_COORDS_PORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGET_COORDS_PORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_GRIPPER_PORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DT12\\Documents\\DT12\\DesignTeam12\\camera-robot-control\\robot_control_functions.py:306\u001b[0m, in \u001b[0;36mmove_to_hand\u001b[1;34m(home, pipeline, MOVE_COORDS_PORT, GET_COORDS_PORT, MOVE_GRIPPER_PORT, hands, HOST)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m depth_frame \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m color_frame:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m indexpoint_3d_mm, wristpoint_3d_mm \u001b[38;5;241m=\u001b[39m \u001b[43mget_hand_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m#if indexpoint_3d_mm is not None and wristpoint_3d_mm is not None:\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# angle = get_hand_angles(indexpoint_3d_mm, wristpoint_3d_mm)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m#    get_hand_angles(indexpoint_3d_mm, wristpoint_3d_mm)\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# If no hand is detected, reset stability and count missing frames.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexpoint_3d_mm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DT12\\Documents\\DT12\\DesignTeam12\\camera-robot-control\\robot_control_functions.py:194\u001b[0m, in \u001b[0;36mget_hand_coords\u001b[1;34m(color_frame, depth_frame, hands)\u001b[0m\n\u001b[0;32m    191\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(color_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Process image for hand landmarks using the global 'hands' object\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\DT12\\anaconda3\\envs\\dt12\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DT12\\anaconda3\\envs\\dt12\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while True:\n",
    "    home = [90, 0, 0, -90, 0, -45]\n",
    "    handHome = [-90, 0, 0, -90, 0, -45]\n",
    "    home[3] = home[3] + random.randint(0, 1)\n",
    "    \n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\n",
    "    time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\n",
    "    inst_img = get_camera_img(pipeline) # get png of tray to run instrument id\n",
    "\n",
    "    #command = get_voice_command(porcupine, cobra, recorder) # get voice command\n",
    "    #inst = get_instrument_name(command) # transcribe voice command and get name of instrument\n",
    "    #instruments = get_instrument_name()\n",
    "    #inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\n",
    "    inst = 'scissors'\n",
    "    x_mid, y_mid = identify_instrument(inst_model, inst_img, inst) # get 2d coords of instrument\n",
    "\n",
    "    ## Get color and depth frame\n",
    "    frames = pipeline.poll_for_frames()\n",
    "    if not frames:\n",
    "        continue\n",
    "\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert yolo instrument coords to 3D coords\n",
    "    inst_coords = get_inst_coords(color_frame, depth_frame, x_mid, y_mid)\n",
    "    if(np.all(inst_coords == 0)):\n",
    "        continue\n",
    "\n",
    "    # target_coords = inst_coords + [home[3], home[4], home[5]]\n",
    "    coords = get_coords(HOST, GET_COORDS_PORT)\n",
    "    if coords is None:\n",
    "            print(\"Robot did not return coordinates\")\n",
    "            continue\n",
    "    \n",
    "    curr_coords = list(coords)\n",
    "    target_coords = transform_camera_to_robot(inst_coords, curr_coords[:3], curr_coords[3:])\n",
    "    target_coords = target_coords + [10, 0, 0]\n",
    "    target_coords = list(target_coords) + [180, 0, 45]\n",
    "\n",
    "    pickSequence(target_coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT)\n",
    "    time.sleep(2)\n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1)\n",
    "    time.sleep(2)\n",
    "    send_coords(handHome, HOST, MOVE_COORDS_PORT, 1)\n",
    "    time.sleep(2)\n",
    "    move_to_hand(handHome, pipeline, MOVE_COORDS_PORT, GET_COORDS_PORT, MOVE_GRIPPER_PORT, hands, HOST)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
