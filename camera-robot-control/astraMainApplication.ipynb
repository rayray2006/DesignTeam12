{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument ID, arm, camera, mediapipe set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-21 Python-3.9.21 torch-2.6.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import struct\n",
    "import socket\n",
    "import torch\n",
    "import random\n",
    "# Import modules from voice-control-instrument-id/voice_instrument_functions.py\n",
    "#instrument_module_path = os.path.abspath(os.path.join(__file__, \"..\", \"..\", \"voice-control-instrument-id\"))\n",
    "notebook_path = os.getcwd()  # Current working directory\n",
    "instrument_module_path = os.path.abspath(os.path.join(notebook_path, \"..\", \"voice-control-instrument-id\"))\n",
    "sys.path.append(instrument_module_path) \n",
    "from voice_instrument_functions import *\n",
    "from audio_utils import *\n",
    "\n",
    "# Load instrument identification model\n",
    "inst_model = load_model('../voice-control-instrument-id/models/instrument_detector_model.pt', False)\n",
    "\n",
    "from robot_control_functions import * # Because of mediapipe, need to import AFTER loading instrument id model\n",
    "\n",
    "# Set up MyCoBot 280\n",
    "HOST = \"10.42.0.1\"\n",
    "GET_COORDS_PORT = 5006\n",
    "MOVE_COORDS_PORT = 5005\n",
    "MOVE_GRIPPER_PORT = 5007\n",
    "home = [90, 0, 0, -90, 0, -45]\n",
    "handHome = [[90, 0, 0, 90, 0, -45]]\n",
    "\n",
    "# Set up Mediapipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Set up depth camera\n",
    "# TODO: open as a window for Design Day\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "#inst_img = get_camera_img(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activation set up - ONLY RUN THIS IF YOU WANT TO SET A CUSTOM VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyttsx3.engine.Engine object at 0x000001228D7F1E80>\n",
      "\n",
      "Available English TTS Voices:\n",
      "[0] Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "[1] Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n",
      "Selected voice: Microsoft David Desktop - English (United States)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choose_tts_voice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (217, 150) | Confidence: 0.31 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "Gripper command sent: 0 50\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (167, 192) | Confidence: 0.88 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "Gripper command sent: 0 50\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (165, 158) | Confidence: 0.44 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m target_coords[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     38\u001b[0m target_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(target_coords) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m45\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m \u001b[43mpickSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_COORDS_PORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_GRIPPER_PORT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DT12\\Documents\\DT12\\DesignTeam12\\camera-robot-control\\robot_control_functions.py:287\u001b[0m, in \u001b[0;36mpickSequence\u001b[1;34m(coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT)\u001b[0m\n\u001b[0;32m    285\u001b[0m coords[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m coords[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[0;32m    286\u001b[0m send_coords(coords, HOST, MOVE_COORDS_PORT)\n\u001b[1;32m--> 287\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m send_gripper_command(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m, HOST, MOVE_GRIPPER_PORT)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    home = [90, 0, 0, -90, 0, -45]\n",
    "    home[3] = home[3] + random.randint(0, 1)\n",
    "    \n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\n",
    "    time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\n",
    "    inst_img = get_camera_img(pipeline) # get png of tray to run instrument id\n",
    "\n",
    "    #command = get_voice_command(porcupine, cobra, recorder) # get voice command\n",
    "    #inst = get_instrument_name(command) # transcribe voice command and get name of instrument\n",
    "    #instruments = get_instrument_name()\n",
    "    #inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\n",
    "    inst = 'scissors'\n",
    "    x_mid, y_mid = identify_instrument(inst_model, inst_img, inst) # get 2d coords of instrument\n",
    "\n",
    "    ## Get color and depth frame\n",
    "    frames = pipeline.poll_for_frames()\n",
    "    if not frames:\n",
    "        continue\n",
    "\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert yolo instrument coords to 3D coords\n",
    "    inst_coords = get_inst_coords(color_frame, depth_frame, x_mid, y_mid)\n",
    "    if(np.all(inst_coords == 0)):\n",
    "        continue\n",
    "\n",
    "    # target_coords = inst_coords + [home[3], home[4], home[5]]\n",
    "    coords = get_coords(HOST, GET_COORDS_PORT)\n",
    "    if coords is None:\n",
    "            print(\"Robot did not return coordinates\")\n",
    "            continue\n",
    "    \n",
    "    curr_coords = list(coords)\n",
    "    target_coords = transform_camera_to_robot(inst_coords, curr_coords[:3], curr_coords[3:])\n",
    "    target_coords[2] = 122\n",
    "    target_coords = list(target_coords) + [180, 10, 45]\n",
    "\n",
    "    pickSequence(target_coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT)\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
