{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument ID, arm, camera, mediapipe set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\maxis/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-17 Python-3.9.21 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import struct\n",
    "import socket\n",
    "import torch\n",
    "# Import modules from voice-control-instrument-id/voice_instrument_functions.py\n",
    "#instrument_module_path = os.path.abspath(os.path.join(__file__, \"..\", \"..\", \"voice-control-instrument-id\"))\n",
    "notebook_path = os.getcwd()  # Current working directory\n",
    "instrument_module_path = os.path.abspath(os.path.join(notebook_path, \"..\", \"voice-control-instrument-id\"))\n",
    "sys.path.append(instrument_module_path) \n",
    "from voice_instrument_functions import *\n",
    "from audio_utils import *\n",
    "\n",
    "# Load instrument identification model\n",
    "inst_model = load_model('../voice-control-instrument-id/models/instrument_detector_model.pt', False)\n",
    "\n",
    "from robot_control_functions import * # Because of mediapipe, need to import AFTER loading instrument id model\n",
    "\n",
    "# Set up MyCoBot 280\n",
    "HOST = \"10.42.0.1\"\n",
    "GET_COORDS_PORT = 5006\n",
    "MOVE_COORDS_PORT = 5005\n",
    "MOVE_GRIPPER_PORT = 5007\n",
    "home = [90, 0, 0, -90, 0, -45]\n",
    "\n",
    "# Set up Mediapipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Set up depth camera\n",
    "# TODO: open as a window for Design Day\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "#inst_img = get_camera_img(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activation set up - ONLY RUN THIS IF YOU WANT TO SET A CUSTOM VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyttsx3.engine.Engine object at 0x000001228D7F1E80>\n",
      "\n",
      "Available English TTS Voices:\n",
      "[0] Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "[1] Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n",
      "Selected voice: Microsoft David Desktop - English (United States)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choose_tts_voice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from robot: \n",
      "\n",
      "Entering live mode. Say 'astra' to begin. Then give a command or say it together like 'astra give me scalpel'\n",
      "Waiting...\n",
      "Heard: brahimia that. thank you.\n",
      "Waiting...\n",
      "Heard:  um, what? anything at the end of the episode? astro!\n",
      "Waiting...\n",
      "Heard:  astrah! yeah, so there's probably something we can code into like threshold the background as well.\n",
      "Waiting...\n",
      "Heard:  yeah, but where is the courage that we're having?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m inst_img \u001b[38;5;241m=\u001b[39m get_camera_img(pipeline) \u001b[38;5;66;03m# get png of tray to run instrument id\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#command = get_voice_command(porcupine, cobra, recorder) # get voice command\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#inst = get_instrument_name(command) # transcribe voice command and get name of instrument\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m instruments \u001b[38;5;241m=\u001b[39m \u001b[43mget_instrument_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m inst \u001b[38;5;241m=\u001b[39m instruments[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\u001b[39;00m\n\u001b[0;32m     10\u001b[0m x_mid, y_mid \u001b[38;5;241m=\u001b[39m identify_instrument(inst_model, inst_img, inst) \u001b[38;5;66;03m# get 2d coords of instrument\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maxis\\Desktop\\JHU\\Term 2\\Design Team\\DesignTeam12\\voice-control-instrument-id\\voice_instrument_functions.py:49\u001b[0m, in \u001b[0;36mget_instrument_name\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     done, instruments \u001b[38;5;241m=\u001b[39m \u001b[43mlisten_and_transcribe_live\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madaptive_phrase_time_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Backward compatibility fallback\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     done \u001b[38;5;241m=\u001b[39m listen_and_transcribe_live()\n",
      "File \u001b[1;32mc:\\Users\\maxis\\Desktop\\JHU\\Term 2\\Design Team\\DesignTeam12\\voice-control-instrument-id\\audio_utils.py:229\u001b[0m, in \u001b[0;36mlisten_and_transcribe_live\u001b[1;34m(phrase_time_limit)\u001b[0m\n\u001b[0;32m    226\u001b[0m issued_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()  \u001b[38;5;66;03m# âœ… Track issued tools for cancellation\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m microphone \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m    230\u001b[0m         recognizer\u001b[38;5;241m.\u001b[39madjust_for_ambient_noise(source)\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\speech_recognition\\__init__.py:175\u001b[0m, in \u001b[0;36mMicrophone.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis audio source is already inside a context manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m Microphone\u001b[38;5;241m.\u001b[39mMicrophoneStream(\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m    179\u001b[0m                 input_device_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat,\n\u001b[0;32m    180\u001b[0m                 rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAMPLE_RATE, frames_per_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCHUNK, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    181\u001b[0m             )\n\u001b[0;32m    182\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\pyaudio\\__init__.py:591\u001b[0m, in \u001b[0;36mPyAudio.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize PortAudio.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m     \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\n",
    "    time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\n",
    "    inst_img = get_camera_img(pipeline) # get png of tray to run instrument id\n",
    "\n",
    "    #command = get_voice_command(porcupine, cobra, recorder) # get voice command\n",
    "    #inst = get_instrument_name(command) # transcribe voice command and get name of instrument\n",
    "    #instruments = get_instrument_name()\n",
    "    #inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\n",
    "    inst = 'scissors'\n",
    "    x_mid, y_mid = identify_instrument(inst_model, inst_img, inst) # get 2d coords of instrument\n",
    "\n",
    "    ## Get color and depth frame\n",
    "    frames = pipeline.poll_for_frames()\n",
    "    if not frames:\n",
    "        continue\n",
    "\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert yolo instrument coords to 3D coords\n",
    "    inst_coords = get_inst_coords(color_frame, depth_frame, x_mid, y_mid)\n",
    "    print(inst_coords)\n",
    "\n",
    "    # target_coords = inst_coords + [home[3], home[4], home[5]]\n",
    "    curr_coords = list(get_coords(HOST, GET_COORDS_PORT))\n",
    "    target_coords = transform_camera_to_robot(inst_coords, curr_coords[:3], curr_coords[3:])\n",
    "    target_coords = list(target_coords) + curr_coords[3:]\n",
    "    print(target_coords)\n",
    "\n",
    "    send_coords(target_coords, HOST, MOVE_COORDS_PORT)\n",
    "    time.sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
