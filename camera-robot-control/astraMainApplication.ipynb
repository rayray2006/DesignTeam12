{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument ID, arm, camera, mediapipe set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "Using cache found in C:\\Users\\maxis/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-17 Python-3.9.21 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m config\u001b[38;5;241m.\u001b[39menable_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mdepth, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m, rs\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mz16, \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     45\u001b[0m config\u001b[38;5;241m.\u001b[39menable_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mcolor, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m, rs\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mbgr8, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m profile \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import struct\n",
    "import socket\n",
    "import torch\n",
    "# Import modules from voice-control-instrument-id/voice_instrument_functions.py\n",
    "#instrument_module_path = os.path.abspath(os.path.join(__file__, \"..\", \"..\", \"voice-control-instrument-id\"))\n",
    "notebook_path = os.getcwd()  # Current working directory\n",
    "instrument_module_path = os.path.abspath(os.path.join(notebook_path, \"..\", \"voice-control-instrument-id\"))\n",
    "sys.path.append(instrument_module_path) \n",
    "from voice_instrument_functions import *\n",
    "from audio_utils import *\n",
    "\n",
    "# Load instrument identification model\n",
    "inst_model = load_model('../voice-control-instrument-id/models/instrument_detector_model.pt', False)\n",
    "\n",
    "from robot_control_functions import * # Because of mediapipe, need to import AFTER loading instrument id model\n",
    "\n",
    "# Set up MyCoBot 280\n",
    "HOST = \"10.42.0.1\"\n",
    "GET_COORDS_PORT = 5006\n",
    "MOVE_COORDS_PORT = 5005\n",
    "MOVE_GRIPPER_PORT = 5007\n",
    "home = [90, 0, 0, -90, 0, -45]\n",
    "\n",
    "# Set up Mediapipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Set up depth camera\n",
    "# TODO: open as a window for Design Day\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "#inst_img = get_camera_img(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activation set up - ONLY RUN THIS IF YOU WANT TO SET A CUSTOM VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyttsx3.engine.Engine object at 0x000001228D7F1E80>\n",
      "\n",
      "Available English TTS Voices:\n",
      "[0] Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "[1] Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n",
      "Selected voice: Microsoft David Desktop - English (United States)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choose_tts_voice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entering live mode. Say 'astra' to begin. Then give a command or say it together like 'astra give me scalpel'\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astra give me the scissors\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astra give me the scissors\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astra give me the scissors\n",
      "\n",
      "[DEBUG] Cleaned command: text astra give me the scissors\n",
      "[DEBUG] Filtered words: ['text', 'astra', 'give', 'me', 'the', 'scissors']\n",
      "Instrument identified: scissors (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'scissors': 0.9}\n",
      "Using AI-generated voice: Getting scissors\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  as from or forceps\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  as strong forceps\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  \n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  theres theres\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astronauts scissors\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astra scissors\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astra scissors\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astra scissors\n",
      "\n",
      "[DEBUG] Cleaned command: text astra scissors\n",
      "[DEBUG] Filtered words: ['text', 'astra', 'scissors']\n",
      "Instrument identified: scissors (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'scissors': 0.9}\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  \n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  \n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astral scissors\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  as grown scissors\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Cleaned command: text astro scissors\n",
      "[DEBUG] Filtered words: ['text', 'scissors']\n",
      "Instrument identified: scissors (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'scissors': 0.9}\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  it\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astro scissors\n",
      "\n",
      "[DEBUG] Cleaned command: text astro scissors\n",
      "[DEBUG] Filtered words: ['text', 'scissors']\n",
      "Instrument identified: scissors (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'scissors': 0.9}\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astro forceps\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astro forceps\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astro forceps\n",
      "\n",
      "[DEBUG] Cleaned command: text astro forceps\n",
      "[DEBUG] Filtered words: ['text', 'forceps']\n",
      "Instrument identified: forceps (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'forceps': 0.9}\n",
      "Using AI-generated voice: Getting forceps\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  the scalpel\n",
      "\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astra scalpel\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astra scalpel\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astra scalpel\n",
      "\n",
      "[DEBUG] Cleaned command: text astra scalpel\n",
      "[DEBUG] Filtered words: ['text', 'astra', 'scalpel']\n",
      "Instrument identified: scalpel (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'scalpel': 0.9}\n",
      "Using AI-generated voice: Getting scalpel\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astra needle\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astra needle\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astra needle\n",
      "\n",
      "[DEBUG] Cleaned command: text astra needle\n",
      "[DEBUG] Filtered words: ['text', 'astra', 'needle']\n",
      "Instrument identified: needle (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'needle': 0.9}\n",
      "Using AI-generated voice: Getting needle\n",
      "Waiting...\n",
      "Heard: \n",
      "  text  astro needle\n",
      "\n",
      "[DEBUG] Raw transcription: \n",
      "  text  astro needle\n",
      "\n",
      "[DEBUG] Cleaned command: \n",
      "  text  astro needle\n",
      "\n",
      "[DEBUG] Cleaned command: text astro needle\n",
      "[DEBUG] Filtered words: ['text', 'needle']\n",
      "Instrument identified: needle (Confidence: 90%)\n",
      "[DEBUG] Final matched instruments with confidence: {'needle': 0.9}\n",
      "Waiting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#command = get_voice_command(porcupine, cobra, recorder) # get voice command\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#inst = get_instrument_name(command) # transcribe voice command and get name of instrument\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     instruments \u001b[38;5;241m=\u001b[39m \u001b[43mget_instrument_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscissors\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maxis\\Desktop\\JHU\\Term 2\\Design Team\\DesignTeam12\\voice-control-instrument-id\\voice_instrument_functions.py:49\u001b[0m, in \u001b[0;36mget_instrument_name\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     done, instruments \u001b[38;5;241m=\u001b[39m \u001b[43mlisten_and_transcribe_live\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madaptive_phrase_time_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Backward compatibility fallback\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     done \u001b[38;5;241m=\u001b[39m listen_and_transcribe_live()\n",
      "File \u001b[1;32mc:\\Users\\maxis\\Desktop\\JHU\\Term 2\\Design Team\\DesignTeam12\\voice-control-instrument-id\\audio_utils.py:307\u001b[0m, in \u001b[0;36mlisten_and_transcribe_live\u001b[1;34m(phrase_time_limit)\u001b[0m\n\u001b[0;32m    305\u001b[0m     recognizer\u001b[38;5;241m.\u001b[39madjust_for_ambient_noise(source)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 307\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_vosk(audio)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\speech_recognition\\__init__.py:491\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistening timed out while waiting for phrase to start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    493\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maxis\\anaconda3\\envs\\dt12\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\n",
    "    time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\n",
    "    inst_img = get_camera_img(pipeline) # get png of tray to run instrument id\n",
    "\n",
    "    #command = get_voice_command(porcupine, cobra, recorder) # get voice command\n",
    "    #inst = get_instrument_name(command) # transcribe voice command and get name of instrument\n",
    "    instruments = get_instrument_name()\n",
    "    #inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\n",
    "    inst = 'scissors'\n",
    "    inst_img = 'inst-camera.png'\n",
    "    x_mid, y_mid = identify_instrument(inst_model, inst_img, inst) # get 2d coords of instrument\n",
    "\n",
    "    ## Get color and depth frame\n",
    "    frames = pipeline.poll_for_frames()\n",
    "    if not frames:\n",
    "        continue\n",
    "\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert yolo instrument coords to 3D coords\n",
    "    inst_coords = get_inst_coords(color_frame, depth_frame, x_mid, y_mid)\n",
    "    print(inst_coords)\n",
    "\n",
    "    # target_coords = inst_coords + [home[3], home[4], home[5]]\n",
    "    curr_coords = list(get_coords(HOST, GET_COORDS_PORT))\n",
    "    target_coords = transform_camera_to_robot(inst_coords, curr_coords[:3], curr_coords[3:])\n",
    "    target_coords = list(target_coords) + curr_coords[3:]\n",
    "    print(target_coords)\n",
    "\n",
    "    send_coords(target_coords, HOST, MOVE_COORDS_PORT)\n",
    "    time.sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
