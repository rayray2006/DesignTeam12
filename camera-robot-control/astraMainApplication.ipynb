{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument ID, arm, camera, mediapipe set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-4-21 Python-3.9.21 torch-2.6.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46167513 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import struct\n",
    "import socket\n",
    "import torch\n",
    "import random\n",
    "# Import modules from voice-control-instrument-id/voice_instrument_functions.py\n",
    "#instrument_module_path = os.path.abspath(os.path.join(__file__, \"..\", \"..\", \"voice-control-instrument-id\"))\n",
    "notebook_path = os.getcwd()  # Current working directory\n",
    "instrument_module_path = os.path.abspath(os.path.join(notebook_path, \"..\", \"voice-control-instrument-id\"))\n",
    "sys.path.append(instrument_module_path) \n",
    "from voice_instrument_functions import *\n",
    "from audio_utils import *\n",
    "\n",
    "# Load instrument identification model\n",
    "inst_model = load_model('../voice-control-instrument-id/models/instrument_detector_model.pt', False)\n",
    "\n",
    "from robot_control_functions import * # Because of mediapipe, need to import AFTER loading instrument id model\n",
    "\n",
    "# Set up MyCoBot 280\n",
    "HOST = \"10.42.0.1\"\n",
    "GET_COORDS_PORT = 5006\n",
    "MOVE_COORDS_PORT = 5005\n",
    "MOVE_GRIPPER_PORT = 5007\n",
    "home = [90, 0, 0, -90, 0, -45]\n",
    "handHome = [[90, 0, 0, 90, 0, -45]]\n",
    "\n",
    "# Set up Mediapipe hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Set up depth camera\n",
    "# TODO: open as a window for Design Day\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "#inst_img = get_camera_img(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activation set up - ONLY RUN THIS IF YOU WANT TO SET A CUSTOM VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyttsx3.engine.Engine object at 0x000001228D7F1E80>\n",
      "\n",
      "Available English TTS Voices:\n",
      "[0] Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "[1] Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n",
      "Selected voice: Microsoft David Desktop - English (United States)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choose_tts_voice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (217, 150) | Confidence: 0.31 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "Gripper command sent: 0 50\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (167, 192) | Confidence: 0.88 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n",
      "Gripper command sent: 0 50\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No instrument found\n",
      "Response from robot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DT12/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box center: (165, 158) | Confidence: 0.44 | Class: Surgical Scissors Sharp/Sharp\n",
      "Gripper command sent: 100 100\n",
      "Response from robot: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m target_coords[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     38\u001b[0m target_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(target_coords) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m45\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m \u001b[43mpickSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_COORDS_PORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMOVE_GRIPPER_PORT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DT12\\Documents\\DT12\\DesignTeam12\\camera-robot-control\\robot_control_functions.py:287\u001b[0m, in \u001b[0;36mpickSequence\u001b[1;34m(coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT)\u001b[0m\n\u001b[0;32m    285\u001b[0m coords[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m coords[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[0;32m    286\u001b[0m send_coords(coords, HOST, MOVE_COORDS_PORT)\n\u001b[1;32m--> 287\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m send_gripper_command(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m, HOST, MOVE_GRIPPER_PORT)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_coords(HOST, GET_COORDS_PORT):\n",
    "    \"\"\"Connects to the robot's coordinates server to retrieve current coordinates.\"\"\"\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.connect((HOST, GET_COORDS_PORT))\n",
    "            # Send the command to get current coordinates.\n",
    "            sock.sendall(b\"GET_COORDS\")\n",
    "            data = sock.recv(1024)\n",
    "            \n",
    "            # Check if the server returned an error.\n",
    "            if data.startswith(b\"ERROR\"):\n",
    "                print(\"Error: Failed to retrieve coordinates.\")\n",
    "                return None\n",
    "            \n",
    "            # Unpack the 6 float values (each float is 4 bytes, so expect 24 bytes total).\n",
    "            if len(data) == struct.calcsize(\"6f\"):\n",
    "                coords = struct.unpack(\"6f\", data)\n",
    "                return coords\n",
    "            else:\n",
    "                print(f\"Unexpected data length: {len(data)} bytes.\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(\"An exception occurred:\", e)\n",
    "        return None\n",
    "\n",
    "def send_gripper_command(state, speed, HOST, MOVE_GRIPPER_PORT):\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.connect((HOST, MOVE_GRIPPER_PORT))\n",
    "            data = struct.pack(\"2f\", state, speed)\n",
    "            sock.sendall(data)\n",
    "            print(\"Gripper command sent:\", state, speed)\n",
    "    except Exception as e:\n",
    "        print(\"Error sending gripper command:\", e)\n",
    "\n",
    "def send_coords(coords, HOST, MOVE_COORDS_PORT, type = 0):\n",
    "    \"\"\"\n",
    "    Connects to the robot's target coordinates server and sends a 6-float binary payload.\n",
    "    \n",
    "    Parameters:\n",
    "        coords (list or tuple): A list or tuple containing 6 float values representing the target coordinates.\n",
    "    \"\"\"\n",
    "    # Pack the coordinates into binary data (6 floats).\n",
    "    data = struct.pack(\"6fi\", *coords, type)\n",
    "    \n",
    "    # Create a socket and connect to the robot's server.\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        try:\n",
    "            sock.connect((HOST, MOVE_COORDS_PORT))\n",
    "            sock.sendall(data)\n",
    "            \n",
    "            # Wait for and print the response from the server.\n",
    "            response = sock.recv(1024)\n",
    "            print(\"Response from robot:\", response.decode())\n",
    "        except Exception as e:\n",
    "            print(\"Error connecting or sending data:\", e)\n",
    "            \n",
    "def euler_to_rotation_matrix(roll, pitch, yaw):\n",
    "    \"\"\"\n",
    "    Create a rotation matrix from Euler angles (roll, pitch, yaw).\n",
    "    Angles are in radians. Using intrinsic rotations in ZYX order.\n",
    "    \"\"\"\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, math.cos(roll), -math.sin(roll)],\n",
    "        [0, math.sin(roll), math.cos(roll)]\n",
    "    ])\n",
    "    \n",
    "    R_y = np.array([\n",
    "        [math.cos(pitch), 0, math.sin(pitch)],\n",
    "        [0, 1, 0],\n",
    "        [-math.sin(pitch), 0, math.cos(pitch)]\n",
    "    ])\n",
    "    \n",
    "    R_z = np.array([\n",
    "        [math.cos(yaw), -math.sin(yaw), 0],\n",
    "        [math.sin(yaw), math.cos(yaw), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Combined rotation: R = R_z * R_y * R_x\n",
    "    return R_z @ R_y @ R_x\n",
    "\n",
    "def transform_camera_to_robot(camera_coords, end_effector_coords, euler_angles, angles_in_degrees=True):\n",
    "    x_c, y_c, z_c = camera_coords\n",
    "    X_ee, Y_ee, Z_ee = end_effector_coords\n",
    "    roll, pitch, yaw = euler_angles\n",
    "\n",
    "    if angles_in_degrees:\n",
    "        roll = math.radians(roll)\n",
    "        pitch = math.radians(pitch)\n",
    "        yaw = math.radians(yaw)\n",
    "    \n",
    "    R_ee = euler_to_rotation_matrix(roll, pitch, yaw)\n",
    "    \n",
    "    # Fixed permutation: new_x = camera_y, new_y = camera_z, new_z = camera_x.\n",
    "    R_fixed = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    camera_vec = np.array([[x_c], [y_c], [z_c]])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    transformed_change = R_ee @ (R_fixed @ camera_vec)\n",
    "    \n",
    "    # Multiply y and z changes by -1 before adding translation.\n",
    "    x_offset = 15  # replace with your desired offset in mm\n",
    "    y_offset = 75\n",
    "    z_offset = 100\n",
    "    transformed_change[0] = transformed_change[0] + transformed_change[0]*0.165\n",
    "    \n",
    "\n",
    "    robot_vec = np.array([[X_ee+ x_offset], [Y_ee  + y_offset], [Z_ee + z_offset]]) + transformed_change\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return robot_vec.flatten()\n",
    "\n",
    "def get_inst_coords(color_frame, depth_frame, x_mid, y_mid):\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    instdepth_value = depth_frame.get_distance(x_mid, y_mid)\n",
    "    color_intrinsics = color_frame.profile.as_video_stream_profile().get_intrinsics()\n",
    "\n",
    "    instpoint_3d = rs.rs2_deproject_pixel_to_point(color_intrinsics,\n",
    "                                                           [x_mid, y_mid],\n",
    "                                                           instdepth_value)\n",
    "    instpoint_3d_mm = [coord * 1000 for coord in instpoint_3d]\n",
    "\n",
    "    theta = math.radians(45)\n",
    "   \n",
    "\n",
    "    # Rotation matrix for a rotation around the z-axis:\n",
    "    R_z = np.array([\n",
    "        [math.cos(theta), -math.sin(theta), 0],\n",
    "        [math.sin(theta),  math.cos(theta), 0],\n",
    "        [0,                0,               1]\n",
    "    ])\n",
    "\n",
    "    # Example coordinate vector\n",
    "    instpoint_3d_mm_new = np.array(instpoint_3d_mm)\n",
    "\n",
    "    # Transform the coordinate using the rotation matrix\n",
    "    insttransformed_coord = R_z @ instpoint_3d_mm_new\n",
    "\n",
    "\n",
    "    return insttransformed_coord\n",
    "\n",
    "def get_hand_coords(color_frame, depth_frame):\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process image for hand landmarks using the global 'hands' object\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            try:\n",
    "                h, w, _ = color_image.shape\n",
    "                \n",
    "                # Use the index finger tip (landmark 9) as an example\n",
    "                index_tip = hand_landmarks.landmark[5]\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                otherFinger = hand_landmarks.landmark[17]\n",
    "                indexpixel_x, indexpixel_y = int(index_tip.x * w), int(index_tip.y * h)\n",
    "                wristpixel_x, wristpixel_y = int(wrist.x * w), int(wrist.y * h)\n",
    "                otherpixel_x, otherpixel_y = int(otherFinger.x * w), int(otherFinger.y * h)\n",
    "                # Ensure pixel coordinates are within image bounds\n",
    "                indexpixel_x = max(0, min(indexpixel_x, w - 1))\n",
    "                indexpixel_y = max(0, min(indexpixel_y, h - 1))\n",
    "                wristpixel_x = max(0, min(wristpixel_x, w - 1))\n",
    "                wristpixel_y = max(0, min(wristpixel_y, h - 1))\n",
    "                otherpixel_x = max(0, min(otherpixel_x, w - 1))\n",
    "                otherpixel_y = max(0, min(otherpixel_y, h - 1))\n",
    "                \n",
    "                # Get the depth at the pixel (in meters)\n",
    "                indexdepth_value = depth_frame.get_distance(indexpixel_x, indexpixel_y)\n",
    "                otherdepth_value = depth_frame.get_distance(otherpixel_x, otherpixel_y)\n",
    "                wristdepth_value = depth_frame.get_distance(wristpixel_x, wristpixel_y)\n",
    "                if indexdepth_value == 0 or wristdepth_value == 0 or otherdepth_value==0:\n",
    "                    continue\n",
    "                wristdepth_value = otherdepth_value\n",
    "                \n",
    "                \n",
    "                # Get intrinsics from the color stream\n",
    "                color_intrinsics = color_frame.profile.as_video_stream_profile().get_intrinsics()\n",
    "                \n",
    "                # Deproject the 2D pixel (with depth) to a 3D point (in meters)\n",
    "                indexpoint_3d = rs.rs2_deproject_pixel_to_point(color_intrinsics,\n",
    "                                                           [indexpixel_x, indexpixel_y],\n",
    "                                                           indexdepth_value)\n",
    "                wristpoint_3d = rs.rs2_deproject_pixel_to_point(color_intrinsics,\n",
    "                                                           [wristpixel_x, wristpixel_y],\n",
    "                                                           wristdepth_value)\n",
    "                # Convert from meters to millimeters\n",
    "                indexpoint_3d_mm = [coord * 1000 for coord in indexpoint_3d]\n",
    "                wristpoint_3d_mm = [coord * 1000 for coord in wristpoint_3d]\n",
    "\n",
    "                theta = math.radians(45)\n",
    "                # indexpoint_3d_mm[0] = indexpoint_3d_mm[0]*1.25\n",
    "                # indexpoint_3d_mm[1] = indexpoint_3d_mm[1]*1.25\n",
    "\n",
    "                # wristpoint_3d_mm[0] = wristpoint_3d_mm[0]*1.25\n",
    "                # wristpoint_3d_mm[1] = wristpoint_3d_mm[1]*1.25\n",
    "                # Rotation matrix for a rotation around the z-axis:\n",
    "                R_z = np.array([\n",
    "                    [math.cos(theta), -math.sin(theta), 0],\n",
    "                    [math.sin(theta),  math.cos(theta), 0],\n",
    "                    [0,                0,               1]\n",
    "                ])\n",
    "\n",
    "                # Example coordinate vector\n",
    "                indexcoord = np.array(indexpoint_3d_mm)\n",
    "                wristcoord = np.array(wristpoint_3d_mm)\n",
    "\n",
    "                # Transform the coordinate using the rotation matrix\n",
    "                indextransformed_coord = R_z @ indexcoord\n",
    "                wristtransformed_coord = R_z @ wristcoord\n",
    "\n",
    "\n",
    "                return indextransformed_coord, wristtransformed_coord\n",
    "            except Exception as e:\n",
    "                print(\"Error processing hand landmarks:\", e)\n",
    "                continue\n",
    "    return None, None\n",
    "def get_hand_angles(indexPoint, wristPoint):\n",
    "    indexx = indexPoint[0]\n",
    "    indexy = indexPoint[1]\n",
    "    wristx = wristPoint[0]\n",
    "    wristy = wristPoint[1]\n",
    "    # Calculate the angle (in radians) between the wrist and index finger relative to the x-axis.\n",
    "    theta = math.atan((indexy - wristy)/(indexx - wristx))\n",
    "    if (indexy - wristy < 0):\n",
    "        theta = math.pi - theta\n",
    "    \n",
    "    # Compute the rotation angle needed to align this line with the y-axis.\n",
    "    rotation_angle = (math.pi / 2) - theta\n",
    "\n",
    "    # Convert the angle to degrees for readability.\n",
    "    rotation_angle_deg = (math.degrees(rotation_angle)) -90\n",
    "    return abs(rotation_angle_deg)\n",
    "def pickSequence(coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT):\n",
    "    send_gripper_command(100, 100, HOST, MOVE_GRIPPER_PORT)\n",
    "    time.sleep(1)\n",
    "    coords[5] = coords[5] - 90\n",
    "    send_coords(coords, HOST, MOVE_COORDS_PORT)\n",
    "    time.sleep(2)\n",
    "    send_gripper_command(0, 50, HOST, MOVE_GRIPPER_PORT)\n",
    "def move_to_hand(home, pipeline):\n",
    "    try:\n",
    "        none_counter = 0  # tracks consecutive frames without hand detection\n",
    "        stable_count = 0  # counts consecutive frames with stable hand coordinates\n",
    "        hand_counter = 0\n",
    "        prev_hand_coord = None  # holds the previous hand coordinate for stability comparison\n",
    "        state = \"home\"\n",
    "\n",
    "        while True:\n",
    "            frames = pipeline.poll_for_frames()\n",
    "            if not frames:\n",
    "                continue\n",
    "\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            if not depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            indexpoint_3d_mm, wristpoint_3d_mm = get_hand_coords(color_frame, depth_frame)\n",
    "            \n",
    "\n",
    "            #if indexpoint_3d_mm is not None and wristpoint_3d_mm is not None:\n",
    "                # angle = get_hand_angles(indexpoint_3d_mm, wristpoint_3d_mm)\n",
    "            #    get_hand_angles(indexpoint_3d_mm, wristpoint_3d_mm)\n",
    "\n",
    "            # If no hand is detected, reset stability and count missing frames.\n",
    "            if indexpoint_3d_mm is None:\n",
    "                none_counter += 1\n",
    "                stable_count = 0\n",
    "                prev_hand_coord = None\n",
    "                if none_counter >= 10:\n",
    "                    print(\"No hand detected for 10 frames. Sending robot home.\")\n",
    "                    send_coords(home, HOST, MOVE_COORDS_PORT, 1)\n",
    "                    prev_hand_coord = None\n",
    "                    state = \"home\"\n",
    "                    none_counter = 0\n",
    "                    current_coords = 0  # reset after sending home\n",
    "                    hand_counter = 0\n",
    "                continue\n",
    "            else:\n",
    "                none_counter = 0\n",
    "            if state==\"hand\":\n",
    "                continue\n",
    "            # Check hand coordinate stability using point_3d_mm.\n",
    "            if prev_hand_coord is None:\n",
    "                prev_hand_coord = indexpoint_3d_mm\n",
    "                stable_count = 1\n",
    "            else:\n",
    "                diff = np.linalg.norm(np.array(indexpoint_3d_mm) - np.array(prev_hand_coord))\n",
    "                if diff <= 10:  # 10 mm = 1 cm threshold\n",
    "                    stable_count += 1\n",
    "                else:\n",
    "                    stable_count = 1  # reset if the hand moves more than 5cm\n",
    "                prev_hand_coord = indexpoint_3d_mm\n",
    "\n",
    "            # Only proceed if we have 10 consecutive stable frames.\n",
    "            if stable_count < 10:\n",
    "                continue\n",
    "\n",
    "            # Once stable for 10 frames, get the robot's current coordinates.\n",
    "            endEffectorCoords = get_coords()\n",
    "            if endEffectorCoords is None:\n",
    "                print(\"Robot did not return coordinates\")\n",
    "                continue\n",
    "\n",
    "            end_effector = endEffectorCoords[:3]\n",
    "            euler_angles = endEffectorCoords[3:]\n",
    "\n",
    "            # Transform the camera coordinates to the robot's coordinate system.\n",
    "\n",
    "            base_coords = transform_camera_to_robot(indexpoint_3d_mm, end_effector, euler_angles, angles_in_degrees=True)\n",
    "            print(indexpoint_3d_mm)\n",
    "            target_coords = np.concatenate((base_coords, euler_angles))\n",
    "            #turn = get_hand_angles(indexpoint_3d_mm, wristpoint_3d_mm)\n",
    "            #rz = home[5]\n",
    "            #if rz + turn >= 170:\n",
    "            #    rz -= turn\n",
    "            #else:\n",
    "            #    rz +=turn\n",
    "            #target_coords[5] = rz\n",
    "            send_coords(target_coords, HOST, MOVE_COORDS_PORT)\n",
    "            time.sleep(4)    \n",
    "                #send_gripper_command(0, 50)\n",
    "                #time.sleep(4) \n",
    "            send_coords(home, HOST, MOVE_COORDS_PORT, 1)       \n",
    "            state = \"home\"\n",
    "                # Reset the stability counter after sending the move command.\n",
    "            stable_count = 0\n",
    "            hand_counter = 0\n",
    "            none_counter = 0\n",
    "            prev_hand_coord = None\n",
    "            print(target_coords)\n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    home = [90, 0, 0, -90, 0, -45]\n",
    "    home[3] = home[3] + random.randint(0, 1)\n",
    "    \n",
    "    send_coords(home, HOST, MOVE_COORDS_PORT, 1) # send robot to home\n",
    "    time.sleep(2) # TODO: better way to wait for arm to be stable before getting frame of tray\n",
    "    inst_img = get_camera_img(pipeline) # get png of tray to run instrument id\n",
    "\n",
    "    #command = get_voice_command(porcupine, cobra, recorder) # get voice command\n",
    "    #inst = get_instrument_name(command) # transcribe voice command and get name of instrument\n",
    "    #instruments = get_instrument_name()\n",
    "    #inst = instruments[0][0] ### HARD CODED TO JUST TAKE THE FIRST INSTRUMENT IN THE LIST\n",
    "    inst = 'scissors'\n",
    "    x_mid, y_mid = identify_instrument(inst_model, inst_img, inst) # get 2d coords of instrument\n",
    "\n",
    "    ## Get color and depth frame\n",
    "    frames = pipeline.poll_for_frames()\n",
    "    if not frames:\n",
    "        continue\n",
    "\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    # Convert yolo instrument coords to 3D coords\n",
    "    inst_coords = get_inst_coords(color_frame, depth_frame, x_mid, y_mid)\n",
    "    if(np.all(inst_coords == 0)):\n",
    "        continue\n",
    "\n",
    "    # target_coords = inst_coords + [home[3], home[4], home[5]]\n",
    "    coords = get_coords(HOST, GET_COORDS_PORT)\n",
    "    if coords is None:\n",
    "            print(\"Robot did not return coordinates\")\n",
    "            continue\n",
    "    \n",
    "    curr_coords = list(coords)\n",
    "    target_coords = transform_camera_to_robot(inst_coords, curr_coords[:3], curr_coords[3:])\n",
    "    target_coords[2] = 122\n",
    "    target_coords = list(target_coords) + [180, 10, 45]\n",
    "\n",
    "    pickSequence(target_coords, HOST, MOVE_COORDS_PORT, MOVE_GRIPPER_PORT)\n",
    "    time.sleep(2)\n",
    "    move_to_hand(home, pipeline)\n",
    "    time.sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
